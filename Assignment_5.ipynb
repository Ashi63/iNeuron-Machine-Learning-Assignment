{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b6f5a9d",
   "metadata": {},
   "source": [
    "### 1. What are the key tasks that machine learning do? What does data pre-processing imply?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f802db5c",
   "metadata": {},
   "source": [
    "- Machine learning (ML) can be used to perform a wide range of tasks, but some of the most common ones include:\n",
    "\n",
    "    - Classification: predicting the class or category of a new data point based on previous examples with known classes.\n",
    "\n",
    "    - Regression: predicting a numerical value or continuous variable based on previous examples with known values.\n",
    "\n",
    "    - Clustering: grouping similar data points together based on their features or characteristics.\n",
    "\n",
    "- Data pre-processing refers to the steps taken to prepare raw data for use in ML algorithms.\n",
    "    - This involves a variety of tasks, such as cleaning and transforming data to ensure it is in the right format, dealing with missing or noisy data, normalizing or standardizing data to remove bias, and selecting relevant features or variables. \n",
    "    - Data pre-processing is a critical step in ML because it can greatly affect the accuracy and performance of the resulting models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24072b58",
   "metadata": {},
   "source": [
    "### 2. Describe quantitative and qualitative data in depth. Make a distinction between the two."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5b5325",
   "metadata": {},
   "source": [
    "- Quantitative and qualitative data are two types of data that are commonly used in research and analysis. \n",
    "- Quantitative data is numeric and involves measurements, while qualitative data is non-numeric and involves observations.\n",
    "\n",
    "    - Quantitative data refers to data that can be measured and expressed numerically. This type of data can be collected through various methods such as surveys, experiments, or observations. \n",
    "    - Examples of quantitative data include height, weight, age, temperature, and scores on tests or exams. Quantitative data can be analyzed using statistical methods such as regression analysis, correlation analysis, and hypothesis testing.\n",
    "\n",
    "    - On the other hand, qualitative data refers to data that is descriptive and non-numeric. This type of data can be collected through methods such as interviews, focus groups, and observations. \n",
    "    - Examples of qualitative data include opinions, attitudes, beliefs, and experiences. Qualitative data can be analyzed using techniques such as content analysis, thematic analysis, and discourse analysis.\n",
    "\n",
    "The main difference between quantitative and qualitative data is that quantitative data involves numeric measurements while qualitative data involves non-numeric observations. \n",
    "\n",
    "Quantitative data is objective and can be analyzed using statistical methods, while qualitative data is subjective and requires a more interpretive approach. In addition, quantitative data is generally more structured and can be easily quantified, while qualitative data is less structured and often requires more effort to categorize and interpret.\n",
    "\n",
    "Both quantitative and qualitative data have their own strengths and weaknesses, and they can be used in combination to provide a more comprehensive understanding of a research question or problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053c8e68",
   "metadata": {},
   "source": [
    "### 3. Create a basic data collection that includes some sample records. Have at least one attribute from each of the machine learning data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "721b4447",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba3dc462",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_name = ['Name','Age','Gender','Income','Education Level','Job Type']\n",
    "fisrt_row = ['John',25,'Male',50000,\"Bachelor's\",'Engineer']\n",
    "second_row = ['Alex',45,'Female',40000,\"Master's\",'Lawyer']\n",
    "third_row = ['Emily',28,'Female',32000,\"High School\",'Retail']\n",
    "fourth_row = ['Michael',35,'Male',60000,\"Doctorate\",'Doctor']\n",
    "df = pd.DataFrame(data=[fisrt_row,second_row,third_row,fourth_row],columns=column_name)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60d0cde",
   "metadata": {},
   "source": [
    "In this example, we have five sample records with attributes from each of the four machine learning data types:\n",
    "\n",
    "- Numeric data: Age, Income\n",
    "\n",
    "- Categorical data: Gender, Education Level, Job Type\n",
    "\n",
    "The data collection includes information on the individuals' names, ages, genders, incomes, education levels, and job types. These attributes could be used in various machine learning tasks, such as predicting an individual's income based on their education level and job type or clustering individuals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50ddeef",
   "metadata": {},
   "source": [
    "### 4. What are the various causes of machine learning data issues? What are the ramifications?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953dd00b",
   "metadata": {},
   "source": [
    "- There are several causes of machine learning data issues, including:\n",
    "\n",
    "1. Incomplete or missing data: When data is not available or is incomplete, it can result in biased or inaccurate models.\n",
    "\n",
    "2. Noisy or inconsistent data: Data that contains errors, outliers, or inconsistencies can negatively impact the accuracy of the models.\n",
    "\n",
    "3. Biased data: Biases in the data can lead to biased models, which can perpetuate discrimination or unfairness.\n",
    "\n",
    "4. Imbalanced data: When the data is imbalanced, meaning that there are significantly more examples of one class than the other, the models can become biased towards the majority class.\n",
    "\n",
    "5. Irrelevant features: Including irrelevant features or variables in the data can reduce the model's accuracy and increase computational complexity.\n",
    "\n",
    "- The ramifications of machine learning data issues can be severe and can have significant consequences for the accuracy and fairness of the resulting models.\n",
    "    - If the data is incomplete, noisy, or inconsistent, the models may not accurately represent the real-world problem or may produce incorrect predictions.\n",
    "    - Biased or imbalanced data can result in models that perpetuate discrimination or unfairly disadvantage certain groups.\n",
    "    - Inaccurate models can have significant implications in areas such as healthcare, finance, and criminal justice, where the decisions made based on the models can have serious consequences for individuals and society as a whole. \n",
    "    - Additionally, irrelevant features can lead to overfitting and reduce the models' generalizability, making them less useful for new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebbdd5e",
   "metadata": {},
   "source": [
    "### 5. Demonstrate various approaches to categorical data exploration with appropriate examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20ded07",
   "metadata": {},
   "source": [
    "- Exploring categorical data is an important step in understanding the distribution and relationships between different categories. Here are a few approaches to explore categorical data:\n",
    "\n",
    "1. Frequency table: A frequency table summarizes the distribution of categorical variables by showing the number or percentage of observations in each category. For example, consider a survey asking people about their favorite type of fruit. A frequency table could show the number of people who chose each fruit.\n",
    "\n",
    "2. Bar chart: A bar chart is a graphical representation of the frequency table, with each category represented by a bar. The height of each bar represents the frequency or percentage of observations in that category.\n",
    "\n",
    "3. Pie chart: A pie chart is another way to represent the frequency distribution of categorical data. Each category is represented by a slice of the pie, with the size of the slice proportional to the frequency or percentage of observations in that category.\n",
    "\n",
    "4. Stacked bar chart: A stacked bar chart is useful for comparing the frequency distribution of a categorical variable across multiple groups or subgroups. Each bar is divided into segments representing the proportion of observations in each category for each group or subgroup."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12de0d09",
   "metadata": {},
   "source": [
    "### 6. How would the learning activity be affected if certain variables have missing values? Having said that, what can be done about it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b54ad6",
   "metadata": {},
   "source": [
    "If certain variables have missing values, the learning activity can be affected in several ways:\n",
    "\n",
    "- Reduced sample size: If a significant portion of the data is missing, it can reduce the sample size and the statistical power of the analysis. This can lead to decreased accuracy and reliability of the results.\n",
    "\n",
    "- Biased estimates: Missing data can introduce bias into the estimates of the model parameters, particularly if the missing data is not missing at random (MNAR) or missing completely at random (MCAR).\n",
    "\n",
    "- Reduced precision: Missing data can lead to reduced precision and increased variability in the estimates of the model parameters, which can lead to decreased confidence in the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b80583a",
   "metadata": {},
   "source": [
    "In machine learning, missing values can significantly impact the performance of the models. Therefore, it is essential to handle missing values before training a machine learning model. Here are some common techniques to treat missing values in machine learning:\n",
    "\n",
    "- Deletion: The simplest approach is to delete the rows or columns with missing values. This can be done through listwise deletion, pairwise deletion, or complete case analysis. However, this approach can lead to significant data loss and reduce the accuracy of the model.\n",
    "\n",
    "- Imputation: This involves replacing the missing values with a predicted value. There are various imputation techniques such as mean imputation, median imputation, mode imputation, regression imputation, K-nearest neighbor (KNN) imputation, and multiple imputation. The choice of imputation technique depends on the nature of the data and the missing value pattern.\n",
    "\n",
    "- Using Algorithms that can handle missing values: Some machine learning algorithms can handle missing values by themselves. For instance, decision trees can handle missing values by assigning an estimated value to the missing feature. Similarly, KNN imputation can be used with KNN-based algorithms.\n",
    "\n",
    "- Creating a new category: For categorical data, we can create a new category for missing values. This approach is useful when the missing values represent a separate category or are likely to have a unique relationship with the response variable.\n",
    "\n",
    "- Feature engineering: We can create new features from existing features that do not have missing values. These features can capture the patterns in the missing values and help in improving the model's performance.\n",
    "\n",
    "In summary, there are various techniques for handling missing values in machine learning. It is crucial to carefully evaluate the missing value pattern and choose the most appropriate technique that best fits the data and model's requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5620bd0",
   "metadata": {},
   "source": [
    "### 7. Describe the various methods for dealing with missing data values in depth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8e341b",
   "metadata": {},
   "source": [
    "Dealing with missing data is a common problem in data analysis, and there are several methods for handling it. The choice of method depends on the type and amount of missing data, the objectives of the analysis, and the specific characteristics of the data set. In this answer, I will describe some common methods for dealing with missing data in detail.\n",
    "\n",
    "Listwise deletion or Complete Case Analysis (CCA):\n",
    "Listwise deletion or CCA involves deleting all cases with missing data from the analysis. This is the simplest approach, but it can lead to a loss of data and reduce the statistical power of the analysis. It is appropriate when the missing data is small and randomly distributed. If the missingness is related to the outcome variable or is dependent on other variables in the data set, then listwise deletion can lead to biased results.\n",
    "\n",
    "Pairwise deletion:\n",
    "Pairwise deletion is an alternative to listwise deletion. It involves deleting cases that have missing values only for the variables used in a specific analysis. This approach allows more data to be included in the analysis, but it can lead to biased results if the missing values are not missing completely at random (MCAR).\n",
    "\n",
    "Mean imputation:\n",
    "Mean imputation involves replacing the missing values with the mean value of the available data for that variable. This approach is straightforward, but it can lead to biased results if the missingness is related to the outcome variable or other variables in the data set.\n",
    "\n",
    "Regression imputation:\n",
    "Regression imputation involves using a regression model to predict the missing values. A regression model is built using the variables with complete data to predict the missing value for a particular variable. This approach can lead to more accurate imputation than mean imputation, but it assumes that the missingness is MCAR or missing at random (MAR).\n",
    "\n",
    "Hot-deck imputation:\n",
    "Hot-deck imputation involves replacing missing values with values from a similar non-missing case in the data set. This approach is based on the assumption that similar cases have similar values for the variable of interest. This approach can be useful in situations where the data set has a clear structure, and the missing values are few.\n",
    "\n",
    "Multiple imputation:\n",
    "Multiple imputation involves creating multiple imputed data sets, where missing values are replaced with plausible values based on the observed data. Multiple imputation allows for the uncertainty of the imputed values to be accounted for in the analysis, which can lead to more accurate estimates of the model parameters. Multiple imputation is often considered the gold standard for handling missing data, but it can be computationally intensive and requires a larger sample size.\n",
    "\n",
    "Maximum likelihood estimation:\n",
    "Maximum likelihood estimation (MLE) is a method for estimating model parameters using incomplete data. MLE assumes that the missing values are missing at random (MAR) and maximizes the likelihood function to estimate the model parameters. MLE is an efficient method for handling missing data, but it requires a large sample size and can be computationally intensive.\n",
    "\n",
    "In conclusion, there are several methods for dealing with missing data, each with its advantages and limitations. The choice of method depends on the characteristics of the data set, the type and amount of missing data, and the objectives of the analysis. It is important to carefully evaluate the missing data pattern and choose an appropriate method that best fits the data and analysis requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac335677",
   "metadata": {},
   "source": [
    "### 8. What are the various data pre-processing techniques? Explain dimensionality reduction and function selection in a few words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b80ca5",
   "metadata": {},
   "source": [
    "Data pre-processing is a crucial step in data analysis and machine learning. It involves cleaning, transforming, and preparing the raw data before it can be used for analysis or training models. Here are some common data pre-processing techniques:\n",
    "\n",
    "Data Cleaning:\n",
    "Data cleaning involves removing or correcting data that is invalid, irrelevant, or inconsistent. This includes removing duplicates, correcting typos and spelling errors, dealing with missing or invalid data, and dealing with outliers.\n",
    "\n",
    "Data Transformation:\n",
    "Data transformation involves converting data from one form to another to make it more suitable for analysis or modeling. This includes scaling, normalization, and encoding categorical variables. Scaling and normalization ensure that all features are on the same scale, which can improve the performance of some machine learning algorithms. Encoding categorical variables involves converting categorical data into numerical data so that it can be used in models.\n",
    "\n",
    "Feature Selection:\n",
    "Feature selection involves selecting the most relevant features for the analysis or modeling. This helps to reduce the dimensionality of the data, which can improve the performance of some machine learning algorithms and reduce the risk of overfitting. Feature selection can be done using various methods such as correlation analysis, feature importance, and principal component analysis.\n",
    "\n",
    "Feature Engineering:\n",
    "Feature engineering involves creating new features from existing features to improve the performance of the model. This includes deriving new features from existing features, combining features, and creating interaction terms. Feature engineering can improve the model's ability to capture complex relationships between the features and the outcome variable.\n",
    "\n",
    "Data Integration:\n",
    "Data integration involves combining data from multiple sources into a single dataset. This includes merging datasets, resolving data conflicts, and handling missing data. Data integration can improve the quality and richness of the data and provide a more comprehensive view of the problem being analyzed.\n",
    "\n",
    "Data Reduction:\n",
    "Data reduction involves reducing the size of the data while retaining the essential information. This includes clustering, sampling, and dimensionality reduction. Clustering involves grouping similar data points together, which can reduce the complexity of the data. Sampling involves selecting a subset of the data to analyze, which can reduce the computational cost of the analysis. Dimensionality reduction involves reducing the number of features while retaining the essential information, which can improve the performance of some machine learning algorithms.\n",
    "\n",
    "In summary, data pre-processing is a crucial step in data analysis and machine learning. It involves cleaning, transforming, and preparing the raw data before it can be used for analysis or training models. The various techniques used for data pre-processing include data cleaning, data transformation, feature selection, feature engineering, data integration, and data reduction. The choice of technique depends on the nature of the data and the objectives of the analysis or modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effcd53c",
   "metadata": {},
   "source": [
    "The main difference between feature selection and dimensionality reduction is that feature selection preserves the original features of the dataset but discards the less important ones, while dimensionality reduction creates a new set of features that are a combination of the original features. Feature selection is useful when the number of features is relatively small, while dimensionality reduction is more effective when dealing with high-dimensional datasets where the number of features is much larger than the number of observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa31a6a5",
   "metadata": {},
   "source": [
    "### 9. Answer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4923d355",
   "metadata": {},
   "source": [
    "#### i. What is the IQR? What criteria are used to assess it?\n",
    "- IQR stands for Interquartile Range. It is a measure of statistical dispersion that represents the spread of the middle 50% of the data. The IQR is calculated by subtracting the first quartile (Q1) from the third quartile (Q3). In other words, IQR = Q3 - Q1.\n",
    "\n",
    "- The IQR is a useful measure because it is less sensitive to outliers than other measures of dispersion, such as the range or standard deviation. It is also commonly used in box plots to visually represent the spread of the data.\n",
    "\n",
    "To assess the IQR, one can use the following criteria:\n",
    "\n",
    "- Identify the minimum and maximum values in the data.\n",
    "- Calculate the median, which is the middle value in the data.\n",
    "- Calculate the first quartile (Q1), which is the median of the lower half of the data.\n",
    "- Calculate the third quartile (Q3), which is the median of the upper half of the data.\n",
    "- Calculate the IQR by subtracting Q1 from Q3: IQR = Q3 - Q1.\n",
    "- Identify any outliers in the data. Outliers are defined as values that are more than 1.5 times the IQR below Q1 or above Q3.\n",
    "\n",
    "The IQR can also be used to detect skewness in the data. If the IQR is small relative to the range, it indicates that the data is more symmetrically distributed. If the IQR is large relative to the range, it indicates that the data is more skewed.\n",
    "\n",
    "Overall, the IQR is a useful measure of statistical dispersion that can be used to identify outliers and assess the skewness of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229b8035",
   "metadata": {},
   "source": [
    "#### ii. Describe the various components of a box plot in detail? When will the lower whisker    surpass the upper whisker in length? How can box plots be used to identify outliers?\n",
    "A box plot, also known as a box and whisker plot, is a graphical representation of the distribution of a dataset. It displays several important measures of the data, including the median, quartiles, range, and outliers. Here are the various components of a box plot:\n",
    "\n",
    "- Median: The median is represented by a line inside the box. It is the value that separates the lower 50% of the data from the upper 50% of the data.\n",
    "\n",
    "- Quartiles: The box represents the interquartile range (IQR), which is the range between the first quartile (Q1) and the third quartile (Q3) of the data. The bottom of the box represents Q1, while the top of the box represents Q3.\n",
    "\n",
    "- Whiskers: The whiskers extend from the box to the minimum and maximum values within 1.5 times the IQR. If any values lie beyond this range, they are represented as outliers.\n",
    "\n",
    "- Outliers: Outliers are represented as individual points beyond the whiskers. They are defined as values that are more than 1.5 times the IQR below Q1 or above Q3.\n",
    "\n",
    "\n",
    "If the lower whisker surpasses the upper whisker in length, it indicates that the data is skewed and that there are more extreme values on the lower end of the distribution. This can happen when the data is highly skewed and has many outliers on the lower end.\n",
    "\n",
    "Box plots are useful for identifying outliers because they provide a visual representation of the range and distribution of the data. Outliers are defined as values that lie beyond the whiskers of the box plot. If there are many outliers, they will be displayed as individual points beyond the whiskers. If there are few outliers, they may not be visible in the plot, but their presence can be inferred from the length of the whiskers. Box plots can also be used to compare the distribution of different datasets and identify differences in their central tendency, spread, and skewness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8f7225",
   "metadata": {},
   "source": [
    "### 10. Make brief notes on any two of the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56697108",
   "metadata": {},
   "source": [
    "#### 1. Data collected at regular intervals\n",
    "\n",
    "When data is collected at regular intervals, it is often referred to as time series data. Time series data is a type of data that is collected over time, at equal or regular intervals. This type of data can be used to analyze trends, patterns, and seasonality in a wide range of applications, including finance, economics, weather forecasting, and more.\n",
    "\n",
    "Some examples of time series data include:\n",
    "\n",
    "Stock prices: Data that is collected daily, weekly, or monthly to analyze trends in stock prices over time.\n",
    "\n",
    "Weather data: Data that is collected daily or hourly to analyze patterns and seasonality in weather patterns.\n",
    "\n",
    "Sales data: Data that is collected monthly or quarterly to analyze trends in sales over time.\n",
    "\n",
    "When analyzing time series data, it is important to consider several factors, including:\n",
    "\n",
    "Seasonality: Seasonality refers to recurring patterns in the data that occur at regular intervals. For example, sales data may show higher sales during the holiday season, while weather data may show higher temperatures during the summer months.\n",
    "\n",
    "Trends: Trends refer to long-term changes in the data over time. For example, stock prices may show an overall upward trend over several years, while sales data may show a gradual decline in sales over time.\n",
    "\n",
    "Cyclical fluctuations: Cyclical fluctuations refer to irregular, repeating patterns in the data that do not occur at regular intervals. For example, the economy may experience periodic cycles of expansion and contraction.\n",
    "\n",
    "To analyze time series data, various statistical and analytical methods can be used, including time series decomposition, autocorrelation analysis, and forecasting models such as ARIMA, exponential smoothing, and neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c703bc3",
   "metadata": {},
   "source": [
    "#### 2. The gap between the quartiles\n",
    "\n",
    "The gap between the quartiles, also known as the interquartile range (IQR), is a measure of the spread or dispersion of a dataset. It is calculated as the difference between the third quartile (Q3) and the first quartile (Q1).\n",
    "\n",
    "The IQR is a useful measure of spread because it is not affected by extreme values or outliers in the data. Unlike the range, which is simply the difference between the maximum and minimum values in the data, the IQR only considers the central 50% of the data, which makes it a more robust measure of spread.\n",
    "\n",
    "To calculate the IQR, the dataset is first sorted in ascending order. Then, the median is calculated to divide the dataset into two halves. The median of the lower half is the first quartile (Q1), and the median of the upper half is the third quartile (Q3). The IQR is then calculated as:\n",
    "\n",
    "IQR = Q3 - Q1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e662e95",
   "metadata": {},
   "source": [
    "#### 3. Use a cross-tab\n",
    "\n",
    "A cross-tab, short for cross-tabulation, is a table that displays the frequency distribution of two or more variables. It is commonly used in data analysis to summarize and compare the relationship between two categorical variables. Here is an example of how a cross-tab can be used:\n",
    "\n",
    "Suppose you have collected data on the gender and smoking habits of a group of individuals. You want to compare the percentage of smokers among males and females. A cross-tab can be used to summarize the data as follows:\n",
    "\n",
    "       Non-Smoker\tSmoker\tTotal\n",
    "\n",
    "Male\t60\t          40\t100\n",
    "\n",
    "Female\t80\t          20\t100\n",
    "\n",
    "Total\t140\t          60\t200\n",
    "\n",
    "This table shows the frequency distribution of smoking habits by gender. The rows represent the gender variable, and the columns represent the smoking habit variable. The table shows that among the 100 males in the sample, 40 are smokers (40%), while among the 100 females, only 20 are smokers (20%). Overall, 30% of the sample are smokers.\n",
    "\n",
    "The cross-tab can also be used to calculate proportions or percentages, which can be useful for comparing the relationship between variables. In this case, we can calculate the percentage of smokers by gender as follows:\n",
    "\n",
    "Percentage of male smokers: 40/100 x 100% = 40%\n",
    "Percentage of female smokers: 20/100 x 100% = 20%\n",
    "This cross-tab shows that there is a higher proportion of smokers among males compared to females. The cross-tab can be further analyzed to identify patterns or relationships between variables, such as whether smoking habits vary by age, income, or other demographic variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b3c20e",
   "metadata": {},
   "source": [
    "### 11. Make a comparison between:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d2ec51",
   "metadata": {},
   "source": [
    "#### 1. Data with nominal and ordinal values\n",
    "\n",
    "Data with nominal values and ordinal values are two different types of categorical data.\n",
    "\n",
    "- Nominal data is a type of categorical data where the categories have no inherent order or ranking. Examples of nominal data include categories such as gender (male or female), eye color (blue, brown, green, etc.), or type of animal (dog, cat, bird, etc.). Nominal data is often represented using frequencies or percentages.\n",
    "\n",
    "- Ordinal data, on the other hand, is a type of categorical data where the categories have an inherent order or ranking. Examples of ordinal data include rankings such as first, second, or third place in a race, or ratings such as poor, fair, good, or excellent. Ordinal data can be represented using either frequencies or percentages, but it is important to keep in mind that the order of the categories matters.\n",
    "\n",
    "When analyzing data with nominal and ordinal values, different statistical techniques may be used. For nominal data, measures of central tendency (such as mean or median) are not useful, but measures of frequency distribution (such as mode) are relevant. Chi-square tests or contingency tables can be used to compare frequencies between categories.\n",
    "\n",
    "For ordinal data, measures of central tendency and dispersion (such as median and interquartile range) are useful, as well as measures of correlation (such as Spearman's rank correlation coefficient). Mann-Whitney U test and Kruskal-Wallis test are also commonly used for analyzing ordinal data.\n",
    "\n",
    "Overall, the type of analysis performed will depend on the research question, the nature of the data, and the statistical techniques available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e72f44b",
   "metadata": {},
   "source": [
    "#### 2. Histogram and box plot\n",
    "\n",
    "Histograms and box plots are both graphical representations used to display the distribution of numerical data. However, they differ in their presentation and the type of information they provide.\n",
    "\n",
    "- A histogram is a graph that displays the distribution of a continuous variable. The data is divided into intervals, or \"bins\", and the number of observations in each bin is represented by the height of a bar. Histograms are useful for showing the shape of the distribution, as well as information about the center and spread of the data. The x-axis represents the range of values, and the y-axis represents the frequency or density of observations within each bin.\n",
    "\n",
    "- A box plot, also known as a box-and-whisker plot, is a graph that displays the distribution of a continuous variable through its quartiles. The box represents the middle 50% of the data, with the bottom and top of the box representing the first and third quartiles, respectively. The line within the box represents the median. The whiskers extend from the box to the minimum and maximum values that are not outliers, and individual data points that fall outside the whiskers are marked as outliers. Box plots are useful for showing the center, spread, and shape of the distribution, as well as identifying outliers.\n",
    "\n",
    "One advantage of box plots over histograms is that they provide more information about the spread of the data, such as the range, quartiles, and outliers. On the other hand, histograms provide more detail about the shape of the distribution, including the presence of multiple peaks or asymmetry.\n",
    "\n",
    "Overall, the choice between a histogram and a box plot will depend on the research question and the type of information that needs to be conveyed. Both can be useful tools for exploring and visualizing the distribution of numerical data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6ac78f",
   "metadata": {},
   "source": [
    "#### 3. The average and median\n",
    "\n",
    "Average and median are two measures of central tendency used to describe the center of a dataset.\n",
    "\n",
    "- The average, also known as the mean, is the sum of all the values in a dataset divided by the number of values. It is a commonly used measure of central tendency, especially for datasets with a symmetrical distribution. However, the mean can be influenced by extreme values or outliers, which can pull the mean away from the center of the data. To calculate the mean, you add up all the values in the dataset and divide by the number of values:\n",
    "\n",
    "Mean = (sum of all values) / (number of values)\n",
    "\n",
    "- The median is the middle value in a dataset when it is arranged in order from smallest to largest (or largest to smallest). It is a robust measure of central tendency that is not affected by extreme values or outliers. For datasets with an odd number of values, the median is the middle value. For datasets with an even number of values, the median is the average of the two middle values. To calculate the median, you need to arrange the dataset in order and locate the middle value:\n",
    "\n",
    "Median = (n+1) / 2th value\n",
    "\n",
    "where n is the number of values in the dataset.\n",
    "\n",
    "In summary, the mean is influenced by extreme values, while the median is not. If a dataset has a symmetrical distribution, the mean and median will be close to each other. However, if the dataset is skewed, the mean and median may differ significantly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4021015e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
