{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b001cda",
   "metadata": {},
   "source": [
    "### 1. What is the underlying concept of Support Vector Machines?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783a12f4",
   "metadata": {},
   "source": [
    "The underlying concept of Support Vector Machines (SVMs) is to find the best possible boundary between two classes in a high-dimensional space. SVMs are a type of supervised learning algorithm used for classification, regression, and outlier detection.\n",
    "\n",
    "The key idea behind SVMs is to find a hyperplane that separates the two classes in such a way that the distance between the hyperplane and the closest data points from each class (known as support vectors) is maximized. This distance is known as the margin, and the goal of SVMs is to find the hyperplane with the maximum margin that still correctly classifies all the training data.\n",
    "\n",
    "SVMs can handle both linear and nonlinear classification problems by using different types of kernels to transform the data into a higher-dimensional space, where it is easier to find a separating hyperplane. Some commonly used kernels are linear, polynomial, and radial basis function (RBF).\n",
    "\n",
    "Overall, the underlying concept of SVMs is to find a hyperplane that maximizes the margin between two classes in a high-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2633e7c8",
   "metadata": {},
   "source": [
    "### 2. What is the concept of a support vector?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4c5482",
   "metadata": {},
   "source": [
    "In Support Vector Machines (SVMs), a support vector is a data point that is closest to the hyperplane that separates the two classes. The hyperplane is chosen in such a way that it maximizes the margin between the two classes, and the support vectors are the data points that lie on the margin or closest to the margin.\n",
    "\n",
    "Support vectors are important because they determine the location of the hyperplane, and changing the position of any other data points in the dataset will not affect the position of the hyperplane as long as they do not cross the margin. This means that SVMs are a type of sparse model, as they only rely on a small subset of the data points, the support vectors, to define the hyperplane.\n",
    "\n",
    "During training, the SVM algorithm seeks to find the hyperplane that maximizes the margin between the two classes, while also correctly classifying all the training data. Once the hyperplane is found, the support vectors can be identified as the data points that are closest to the hyperplane. These support vectors are then used to define the hyperplane and make predictions on new data points.\n",
    "\n",
    "In summary, support vectors are the data points that lie closest to the hyperplane in SVMs and are used to define the position of the hyperplane and make predictions on new data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8607ce9e",
   "metadata": {},
   "source": [
    "### 3. When using SVMs, why is it necessary to scale the inputs?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9394f5f3",
   "metadata": {},
   "source": [
    "When using Support Vector Machines (SVMs), it is necessary to scale the inputs because SVMs are sensitive to the scale of the input features.\n",
    "\n",
    "SVMs aim to find the maximum margin hyperplane that separates the two classes, and the distance between the hyperplane and the support vectors is determined by the input features. If the input features have different scales, then the distances between the hyperplane and the support vectors will also be different, and this can lead to an incorrect hyperplane being selected.\n",
    "\n",
    "For example, if one feature has a much larger scale than the others, it will dominate the distance calculation and may result in the SVM algorithm ignoring the other features. This can lead to poor generalization performance on new data.\n",
    "\n",
    "To avoid these issues, it is important to scale the input features before using them in SVMs. Scaling the input features ensures that all the features have the same scale and range, and this allows the SVM algorithm to treat all the features equally when selecting the hyperplane.\n",
    "\n",
    "Common scaling techniques include standardization (subtracting the mean and dividing by the standard deviation) or normalization (scaling the data to a specific range, such as [0,1]).\n",
    "\n",
    "In summary, scaling the input features is necessary when using SVMs to avoid bias towards features with larger scales and to ensure the SVM algorithm can find the optimal hyperplane."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973a7cda",
   "metadata": {},
   "source": [
    "### 4. When an SVM classifier classifies a case, can it output a confidence score? What about a percentage chance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47ebece",
   "metadata": {},
   "source": [
    "Yes, an SVM classifier can output a confidence score, which is a measure of how confident the classifier is in its prediction for a given case. The confidence score is based on the distance of the case from the decision boundary (hyperplane) in the feature space. The farther away a case is from the hyperplane, the more confident the classifier is in its prediction.\n",
    "\n",
    "The confidence score is sometimes referred to as the \"margin\" or \"distance-to-boundary\" and is a continuous value. It is possible to set a threshold on the confidence score to make binary decisions, such as whether a case belongs to a certain class or not.\n",
    "\n",
    "However, SVMs do not output a percentage chance or probability directly, as they do not model the probability distribution of the data. Instead, SVMs focus on finding the best hyperplane that separates the two classes, based on the distances between the support vectors and the decision boundary.\n",
    "\n",
    "To estimate the probability of a case belonging to a certain class, techniques such as Platt scaling or logistic regression can be used to convert the SVM's confidence scores into probabilities. These techniques involve training a separate model on top of the SVM's decision function to estimate the probability of a case belonging to a class, based on its confidence score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1d83a1",
   "metadata": {},
   "source": [
    "### 5. Should you train a model on a training set with millions of instances and hundreds of features using the primal or dual form of the SVM problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd40d29",
   "metadata": {},
   "source": [
    "When training a model on a large training set with millions of instances and hundreds of features, it is generally recommended to use the dual form of the SVM problem, rather than the primal form.\n",
    "\n",
    "The dual form is more efficient when the number of instances is larger than the number of features, as it involves computing the dot product between pairs of instances, which can be precomputed and stored in a matrix. This matrix is then used to solve the dual optimization problem, which involves finding a set of Lagrange multipliers that maximize the dual objective function subject to some constraints.\n",
    "\n",
    "In contrast, the primal form involves solving an optimization problem with hundreds of thousands or millions of variables, which can be computationally expensive and time-consuming. Therefore, the dual form is generally faster and more efficient for large-scale problems.\n",
    "\n",
    "Moreover, the dual form of SVM allows the use of kernel functions, which can transform the input data into a higher-dimensional feature space, allowing the model to capture more complex relationships between the features. The use of kernel functions is not possible in the primal form.\n",
    "\n",
    "However, for smaller datasets with fewer features, the primal form may be more suitable as it can be more computationally efficient in those cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d045b9",
   "metadata": {},
   "source": [
    "### 6. Let's say you've used an RBF kernel to train an SVM classifier, but it appears to underfit the training collection. Is it better to raise or lower (gamma)? What about the letter C?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131806a6",
   "metadata": {},
   "source": [
    "If an SVM classifier trained with an RBF kernel is underfitting the training set, it means that the model is too simple and cannot capture the underlying patterns in the data. In this case, we can try to adjust the hyperparameters of the model, including the kernel parameters gamma and the regularization parameter C, to improve its performance.\n",
    "\n",
    "When the SVM underfits the data, we can try to increase the gamma parameter. The gamma parameter controls the shape of the RBF kernel, and a higher value of gamma makes the model more sensitive to the training data points, which can help capture more complex relationships between the data points. However, increasing gamma too much can cause overfitting, so it's essential to tune this parameter carefully.\n",
    "\n",
    "Regarding the C parameter, which controls the regularization strength, we can try to decrease it to allow the model to fit the training data more closely. A smaller value of C means that the model is more tolerant of misclassified training points, which can help it capture the underlying patterns in the data. However, too much decrease in C can lead to overfitting and reduced generalization performance on new data.\n",
    "\n",
    "In summary, if an SVM classifier trained with an RBF kernel is underfitting the training set, we can try increasing the gamma parameter and decreasing the C parameter to improve its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c858e10",
   "metadata": {},
   "source": [
    "### 7. To solve the soft margin linear SVM classifier problem with an off-the-shelf QP solver, how should the QP parameters (H, f, A, and b) be set?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69a16b5",
   "metadata": {},
   "source": [
    "To solve the soft margin linear SVM classifier problem with an off-the-shelf QP solver, the QP parameters (H, f, A, and b) should be set as follows:\n",
    "\n",
    "1. The H matrix should be set to an n x n matrix, where n is the number of training instances. The H matrix is a diagonal matrix whose diagonal elements are the product of the corresponding training labels. In other words, H[i][j] = y[i]*y[j], where y[i] is the label of the i-th training instance.\n",
    "\n",
    "2. The f vector should be set to an n-dimensional vector, where n is the number of training instances. The f vector is a vector of -1s, which corresponds to the constant term in the dual form of the SVM optimization problem.\n",
    "\n",
    "3. The A matrix should be set to an m x n matrix, where m is the number of inequality constraints. In the case of the soft margin SVM, there are two inequality constraints per training instance, one for the upper bound and one for the lower bound. The A matrix should be set up such that the first m/2 rows correspond to the upper bound constraints, and the second m/2 rows correspond to the lower bound constraints. Each row of the A matrix should be the transpose of the corresponding feature vector multiplied by the corresponding training label.\n",
    "\n",
    "4. The b vector should be set to an m-dimensional vector, where m is the number of inequality constraints. The b vector should be set up such that the first m/2 elements are the upper bound constraint values, and the second m/2 elements are the lower bound constraint values. The b vector should be a vector of 1s.\n",
    "\n",
    "Once these QP parameters are set up, they can be fed into an off-the-shelf QP solver, such as the quadratic programming solver provided by the CVXOPT library in Python, to obtain the solution to the soft margin SVM classifier problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6575f5d4",
   "metadata": {},
   "source": [
    "### 8. On a linearly separable dataset, train a LinearSVC. Then, using the same dataset, train an SVC and an SGDClassifier. See if you can get them to make a model that is similar to yours."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446e5799",
   "metadata": {},
   "source": [
    "Code below that shows how to train a LinearSVC, an SVC, and an SGDClassifier on a linearly separable dataset using scikit-learn library in Python:\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#### create a linearly separable dataset\n",
    "\n",
    "X, y = make_classification(n_features=2, n_redundant=0, n_informative=1,\n",
    "                           random_state=42, n_clusters_per_class=1)\n",
    "\n",
    "#### split the dataset into training and testing sets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#### train LinearSVC\n",
    "\n",
    "linear_svc = LinearSVC()\n",
    "\n",
    "linear_svc.fit(X_train, y_train)\n",
    "\n",
    "#### train SVC with linear kernel\n",
    "\n",
    "svc = SVC(kernel='linear')\n",
    "\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "#### train SGDClassifier with hinge loss\n",
    "\n",
    "sgd = SGDClassifier(loss='hinge')\n",
    "\n",
    "sgd.fit(X_train, y_train)\n",
    "\n",
    "#### evaluate the models on the testing set\n",
    "\n",
    "linear_svc_pred = linear_svc.predict(X_test)\n",
    "\n",
    "svc_pred = svc.predict(X_test)\n",
    "\n",
    "sgd_pred = sgd.predict(X_test)\n",
    "\n",
    "print('LinearSVC accuracy:', accuracy_score(y_test, linear_svc_pred))\n",
    "\n",
    "print('SVC with linear kernel accuracy:', accuracy_score(y_test, svc_pred))\n",
    "\n",
    "print('SGDClassifier accuracy:', accuracy_score(y_test, sgd_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f5efac",
   "metadata": {},
   "source": [
    "### 9. On the MNIST dataset, train an SVM classifier. You'll need to use one-versus-the-rest to assign all 10 digits because SVM classifiers are binary classifiers. To accelerate up the process, you might want to tune the hyperparameters using small validation sets. What level of precision can you achieve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67e77e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fbaec36f",
   "metadata": {},
   "source": [
    "### 10. On the California housing dataset, train an SVM regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b409f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's start by loading the California housing dataset. You can do that using scikit-learn's fetch_california_housing function:\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "california_housing = fetch_california_housing()\n",
    "X = california_housing.data\n",
    "y = california_housing.target\n",
    "\n",
    "# Now, we can split the data into training and testing sets:\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Next, we can create an instance of the SVR class from scikit-learn's svm module and fit it to the training data:\n",
    "from sklearn.svm import SVR\n",
    "svr = SVR()\n",
    "svr.fit(X_train, y_train)\n",
    "\n",
    "# And that's it! You now have a trained SVM regressor on the California housing dataset. \n",
    "# Of course, you may want to tune the hyperparameters of the SVM to get better performance. \n",
    "# You can do that using techniques such as cross-validation or grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a2ce3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
