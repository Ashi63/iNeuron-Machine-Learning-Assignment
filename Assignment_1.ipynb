{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cac91d78",
   "metadata": {},
   "source": [
    "### 1.What does one mean by the term \"machine learning\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc0827b",
   "metadata": {},
   "source": [
    "- Machine learning is a branch of artificial intelligence (AI) and computer science which focuses on the use of data and algorithms to imitate the way that humans learn, gradually improving its accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5777c464",
   "metadata": {},
   "source": [
    "### 2.Can you think of 4 distinct types of issues where it shines?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ecc675",
   "metadata": {},
   "source": [
    "- Machine Learning is great for complex problems for which we have \n",
    "    - Pattern recognition: Machine learning excels at identifying complex patterns and relationships within large datasets. For example, in image or speech recognition, machine learning algorithms can recognize and classify patterns and features that are difficult for humans to detect.\n",
    "\n",
    "    - Prediction and forecasting: Machine learning can be used to make predictions and forecasts based on historical data. For example, in finance, machine learning algorithms can predict stock prices or identify fraudulent transactions based on historical patterns.\n",
    "\n",
    "    - Natural Language Processing: Machine learning has been used extensively in natural language processing tasks such as sentiment analysis, language translation, speech recognition, and chatbots. Machine learning algorithms can identify patterns in text data and learn to recognize different languages, word meanings, and grammatical structures.\n",
    "\n",
    "    - Recommender systems: Machine learning algorithms are commonly used in recommender systems to personalize product or content recommendations for individual users. By analyzing user behavior and preferences, machine learning algorithms can recommend products or content that are likely to be of interest to each individual user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd2a303",
   "metadata": {},
   "source": [
    "### 3.What is a labeled training set, and how does it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c895a0dc",
   "metadata": {},
   "source": [
    "- A labeled training set is a dataset used in supervised learning, where each example in the dataset is labeled with a corresponding output or target value. In other words, for each input example in the training set, we know the correct output that the algorithm should produce.\n",
    "\n",
    "- The labeled training set is used to train the machine learning algorithm to make accurate predictions on new, unseen data. During training, the algorithm learns to identify patterns and relationships in the input features that are predictive of the output labels. Once trained, the algorithm can be used to make predictions on new, unseen data by applying the learned patterns and relationships to the input features.\n",
    "\n",
    "- For example, in a classification problem where we are trying to predict whether an email is spam or not, the labeled training set would consist of a collection of emails, where each email is labeled with a binary value indicating whether it is spam or not. The machine learning algorithm would be trained on this dataset to learn the patterns and relationships between the words in the email and whether it is spam or not. Once trained, the algorithm could be used to classify new, unseen emails as spam or not spam.\n",
    "\n",
    "- In summary, a labeled training set is an important component of supervised learning, as it provides the machine learning algorithm with the correct outputs for each input example, allowing it to learn to make accurate predictions on new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed461f2",
   "metadata": {},
   "source": [
    "### 4.What are the two most important tasks that are supervised?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a206ec",
   "metadata": {},
   "source": [
    "- Regression.\n",
    "- Classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d90a304",
   "metadata": {},
   "source": [
    "### 5.Can you think of four examples of unsupervised tasks?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fdf83e",
   "metadata": {},
   "source": [
    "1. Clustering: In clustering, the goal is to group similar data points together into clusters without any prior knowledge of what the clusters should be. Clustering can be used in various applications, such as customer segmentation, image segmentation, and anomaly detection.\n",
    "\n",
    "2. Dimensionality Reduction: In dimensionality reduction, the goal is to reduce the number of features in a dataset while preserving the important information. Dimensionality reduction can be used to simplify the data and make it easier to visualize, and can be useful in applications such as image processing, genetics, and natural language processing.\n",
    "\n",
    "3. Anomaly Detection: In anomaly detection, the goal is to identify data points that are significantly different from the rest of the data. Anomaly detection can be used in applications such as fraud detection, intrusion detection, and system health monitoring.\n",
    "\n",
    "4. Association Rule Mining: In association rule mining, the goal is to discover relationships between variables in a dataset. Association rule mining can be used to identify patterns in customer purchase behavior, web navigation behavior, and other applications where identifying relationships between variables is important."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f87b0b",
   "metadata": {},
   "source": [
    "### 6.State the machine learning model that would be best to make a robot walk through various unfamiliar terrains?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c817c3cb",
   "metadata": {},
   "source": [
    "- Reinforcement Learning is likely to perform best if we want a robot to learn to walk in various unknown terrains, since this is typically the type of problem that Reinforcement Learning tackles. \n",
    "- It might be possible to express the problem as a supervised or semisupervised learning problem, but it would be less natural."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3d312a",
   "metadata": {},
   "source": [
    "### 7.Which algorithm will you use to divide your customers into different groups?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9da86d6",
   "metadata": {},
   "source": [
    "- The algorithm used to divide customers into different groups will depend on the specific problem and data at hand, as well as the desired outcome.\n",
    "\n",
    "- One common approach is to use clustering algorithms, which group similar customers together based on their characteristics or behavior. Clustering algorithms aim to identify patterns and relationships in the data without any prior knowledge of what the groups should be.\n",
    "\n",
    "- Some popular clustering algorithms include k-means, hierarchical clustering, and DBSCAN. K-means is a centroid-based algorithm that groups data points based on their proximity to the centroid of the cluster. Hierarchical clustering, on the other hand, creates a hierarchy of nested clusters by iteratively merging or splitting clusters based on their similarity. DBSCAN is a density-based clustering algorithm that groups data points based on their density in the feature space.\n",
    "\n",
    "- Other algorithms that could be used to divide customers into different groups include decision trees, which can be used to identify the most important features for separating different groups of customers, and support vector machines (SVMs), which can be used for both clustering and classification tasks.\n",
    "\n",
    "- Ultimately, the choice of algorithm will depend on the specific problem and the nature of the data, and may require some experimentation and tuning to find the best approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae36a93",
   "metadata": {},
   "source": [
    "### 8.Will you consider the problem of spam detection to be a supervised or unsupervised learning problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c8c03d",
   "metadata": {},
   "source": [
    "- Supervised Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a05b69",
   "metadata": {},
   "source": [
    "### 9.What is the concept of an online learning system?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734d7ee5",
   "metadata": {},
   "source": [
    "- The concept of online learning systems is based on the idea of training machine learning models continuously and dynamically using new data as it becomes available, rather than in a batch mode where all the data is processed at once.\n",
    "\n",
    "- In online learning, the model is updated incrementally as new data arrives, which allows the system to adapt and improve its predictions over time. This is particularly useful in scenarios where the data is non-stationary, meaning that the underlying patterns and relationships in the data change over time. Examples of such scenarios include online recommendation systems, fraud detection systems, and stock market prediction systems.\n",
    "\n",
    "- In an online learning system, the model is initially trained on a small amount of data, and then new data is continuously fed into the system to update the model parameters. The system uses the updated model to make predictions on new incoming data, and the predictions are compared to the actual outcomes to calculate an error signal. The error signal is then used to update the model parameters, and the process repeats iteratively as new data arrives.\n",
    "\n",
    "- Online learning systems are often used in combination with batch learning or offline learning methods. In this approach, the model is first trained offline on a large historical dataset using a batch learning algorithm, and then the updated model is used in an online learning mode to incorporate new incoming data.\n",
    "\n",
    "- Online learning systems have several advantages over batch learning, such as the ability to adapt quickly to changing data, the ability to handle large data streams, and the ability to reduce the computational requirements for training and inference. However, they can be more complex to implement and require careful tuning to prevent overfitting and instability in the model updates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eea78dc",
   "metadata": {},
   "source": [
    "### 10.What is out-of-core learning, and how does it differ from core learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e63bfd",
   "metadata": {},
   "source": [
    "- Machine Learning Algorithms need preprocessing of data, and this data may vary in size.\n",
    "- If we do not have enough computing power to pre-process it, we can not proceed with analyzing it, and then the question is how to load and process such a large amount of data using the available system.\n",
    "- Following are some of the approaches that we can use to handle large amounts of data:\n",
    "\n",
    "    - Sub Sampling (But the main problem with subsampling is data loss. And we know that data is essential in ML, especially when we want to build a good model.)\n",
    "    - Cloud Computing (The main disadvantage of the cloud platform is its Cost.)\n",
    "    - Out of Core ML\n",
    "\n",
    "#### Out-of-Core ML\n",
    "- Out-of-core ML takes the reference from the concept of core memory which is nothing but RAM. We can say that out-of-core memory is only external memory which may be HDD.\n",
    "\n",
    "- If we want to define what is Out of Core ML in layman’s terms, it would be:\n",
    "\n",
    "    - “It is a way to train your model on data that cannot fit your core memory.”\n",
    "\n",
    "- Out-of-core learning refers to the machine learning algorithms working with data that cannot fit into a single machine’s memory but can easily fit into some data storage, such as a local hard disk or web repository.\n",
    "\n",
    "There are three ways to perform it in three steps:\n",
    "\n",
    "   - Streaming data\n",
    "       - In simple terms, streaming data is nothing but loading your data in mini-batches from secondary storage like HDD or web repository into the core memory. For example, if we have 10GB of data, we can divide it into 10 batches of 1 GB each. We will load the first 1GB, process it after processing, removes it from core memory, and load the next batch.\n",
    "\n",
    "       - But to do this, we need a reader. Generally, we call it a data reader. This reader facility is provided by Pandas IO library, which allows us to read data in chunks. In deep learning, Keras has built-in data readers. In Machine Learning, the Vaex library is specifically designed for such operation.\n",
    "\n",
    "   - Extracting features\n",
    "       - Once a chunk of data is in RAM or core memory, we need to extract features from that data. These features we extract from data are used as input for the ML model. This feature extraction is a little tricky part.\n",
    "\n",
    "   - Training model\n",
    "       - In normal conditions, we have all our data in RAM to train our model on that data. But here, we are not able to load all data in memory. Even though we extract features in the second step, those features may be larger than RAM.\n",
    "\n",
    "- Out-of-core learning, also known as online learning, is a type of machine learning that is designed to handle datasets that are too large to fit into memory. In out-of-core learning, the dataset is processed in smaller chunks or batches, with each batch processed one at a time, rather than all at once.\n",
    "\n",
    "- The main difference between out-of-core learning and core learning is in the way the dataset is processed. In core learning, the entire dataset is loaded into memory, and the machine learning algorithm processes the data all at once. However, this approach can be impractical or impossible for very large datasets, such as those encountered in big data applications.\n",
    "\n",
    "- In out-of-core learning, the dataset is processed in smaller batches or chunks, with each batch loaded into memory and processed one at a time. This approach allows the machine learning algorithm to handle datasets that are too large to fit into memory, by processing the data in a more efficient and scalable way.\n",
    "\n",
    "- Out-of-core learning is commonly used in applications such as online advertising, fraud detection, and recommendation systems, where the data is constantly changing and needs to be processed in real-time. It is also commonly used in big data applications, where the size of the dataset exceeds the memory capacity of the computing system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ac59fe",
   "metadata": {},
   "source": [
    "### 11.What kind of learning algorithm makes predictions using a similarity measure?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3478eb55",
   "metadata": {},
   "source": [
    "- A type of learning algorithm that makes predictions using a similarity measure is known as instance-based learning or lazy learning. In instance-based learning, the algorithm does not explicitly learn a model from the training data, but instead, it stores the entire training dataset and makes predictions for new data points by finding the most similar training instances.\n",
    "\n",
    "- The similarity measure used in instance-based learning depends on the type of data and the problem at hand. Common similarity measures include Euclidean distance, cosine similarity, and Pearson correlation coefficient.\n",
    "\n",
    "- K-nearest neighbor (KNN) is a popular instance-based learning algorithm that uses a similarity measure to make predictions. In KNN, the algorithm stores the entire training dataset and, when presented with a new data point, finds the k most similar training instances to the new data point based on the chosen similarity measure. The algorithm then predicts the output value for the new data point based on the average of the output values of the k most similar instances.\n",
    "\n",
    "- Instance-based learning algorithms are useful in cases where the relationship between the input and output variables is complex or difficult to model explicitly. They are also useful when the dataset is small or noisy and when the distribution of the data changes frequently over time. However, they can be computationally expensive and may not perform well when the dataset is large or when there is a high dimensionality in the input space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf0b8e6",
   "metadata": {},
   "source": [
    "### 12.What's the difference between a model parameter and a hyperparameter in a learning algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86947187",
   "metadata": {},
   "source": [
    "- In machine learning, a model is typically trained on a set of input data to make predictions or decisions. \n",
    "- The model consists of two types of parameters: model parameters and hyperparameters.\n",
    "\n",
    "    - Model parameters are learned during the training process and represent the weights or coefficients of the model. These parameters are adjusted automatically by the algorithm to fit the training data and minimize the prediction error. For example, in a linear regression model, the slope and intercept terms are model parameters.\n",
    "\n",
    "    - Hyperparameters, on the other hand, are set before the training process and control the behavior of the algorithm. These parameters cannot be learned from the data directly and must be set by the user. Examples of hyperparameters include the learning rate in gradient descent, the number of hidden layers in a neural network, and the regularization strength in a regularization algorithm.\n",
    "\n",
    "- The choice of hyperparameters can greatly affect the performance of the model, and finding the optimal values is often a trial-and-error process. Hyperparameters are typically tuned using a validation set or through techniques such as grid search, random search, or Bayesian optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183be4b1",
   "metadata": {},
   "source": [
    "### 13.What are the criteria that model-based learning algorithms look for? What is the most popular method they use to achieve success? What method do they use to make predictions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa3361f",
   "metadata": {},
   "source": [
    "- Model-based learning algorithms, also known as generative algorithms, aim to create a model of the underlying structure of the input data in order to make predictions or decisions. The criteria that these algorithms look for depend on the specific task at hand, but generally include:\n",
    "\n",
    "    - Maximizing the likelihood of the data given the model: the model should accurately capture the patterns and distribution of the input data.\n",
    "\n",
    "    - Minimizing the complexity of the model: the model should not be overly complex and should avoid overfitting the training data.\n",
    "\n",
    "    - Balancing bias and variance: the model should be able to generalize well to unseen data while still fitting the training data accurately.\n",
    "\n",
    "- The most popular method used by model-based learning algorithms is probabilistic modeling, which involves estimating the probability distribution of the input data and using it to make predictions or decisions. This approach is used in various models such as Naive Bayes, Hidden Markov Models, and Gaussian Mixture Models.\n",
    "\n",
    "- To make predictions, model-based learning algorithms typically use the model to estimate the probability of each possible outcome, given the input data. The outcome with the highest probability is then chosen as the predicted output. For example, in the case of classification, the algorithm would estimate the probability of each class label for a given input and choose the label with the highest probability as the predicted output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b42519a",
   "metadata": {},
   "source": [
    "### 14.Can you name four of the most important Machine Learning challenges?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a76f79c",
   "metadata": {},
   "source": [
    "- Four of the most important challenges in Machine Learning:\n",
    "\n",
    "    1. Overfitting: Overfitting occurs when a model fits the training data too closely, including noise and other idiosyncrasies of the data, rather than capturing the underlying patterns. This can result in poor generalization to new data.\n",
    "\n",
    "    2. Data quality and quantity: Machine learning algorithms require large amounts of high-quality data to be effective. Poor quality or insufficient data can lead to biased or inaccurate models.\n",
    "\n",
    "    3. Interpretability and explainability: Some machine learning models, such as deep neural networks, can be difficult to interpret and explain. This can make it challenging to understand the reasons behind a model's predictions or decisions.\n",
    "\n",
    "    4. Fairness and bias: Machine learning models can perpetuate or amplify existing biases in the data, leading to unfair or discriminatory outcomes. Ensuring fairness and mitigating bias in machine learning algorithms is an ongoing challenge.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd5a62c",
   "metadata": {},
   "source": [
    "### 15.What happens if the model performs well on the training data but fails to generalize the results to new situations? Can you think of three different options?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0989514d",
   "metadata": {},
   "source": [
    "- If the model performs well on the training data but fails to generalize to new situations, it is said to be overfitting. Here are 5 different options to address this issue:\n",
    "\n",
    "    1. Regularization: Regularization is a technique that adds a penalty term to the loss function of the model, which discourages it from overfitting the data. Regularization techniques include L1 regularization, L2 regularization, and dropout.\n",
    "\n",
    "    2. Cross-validation: Cross-validation is a technique that evaluates the performance of the model on multiple subsets of the data, which helps to identify overfitting. By evaluating the model on multiple subsets of the data, we can get a better estimate of how well the model is likely to perform on new data.\n",
    "\n",
    "    3. Increase training data: One reason why a model might overfit the training data is that the training dataset is too small or not representative of the population. Increasing the size and diversity of the training dataset can help to mitigate overfitting and improve the generalization of the model.\n",
    "\n",
    "    4. Ensemble methods: Ensemble methods combine multiple models to improve their overall performance. By training multiple models on different subsets of the data and combining their predictions, ensemble methods can reduce the impact of overfitting and improve the generalization of the model.\n",
    "\n",
    "    5. Feature selection: Overfitting can also occur when the model is trained on too many features. Feature selection is the process of selecting a subset of the most relevant features for the model, which can improve its generalization performance.\n",
    "\n",
    "Other options to address overfitting may include reducing the complexity of the model, such as reducing the number of hidden layers in a neural network or using a simpler model architecture, or increasing the dropout rate in deep learning models. The choice of which option to use depends on the specific characteristics of the data and the model being used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2622e67",
   "metadata": {},
   "source": [
    "### 16.What exactly is a test set, and why would you need one?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21322ab",
   "metadata": {},
   "source": [
    "- A test set is a dataset that is used to evaluate the performance of a machine learning model after it has been trained on a separate training set. The test set is held out from the training process and is used only once, after the model has been trained, to simulate how well the model would perform on new, unseen data.\n",
    "\n",
    "- The purpose of a test set is to provide an unbiased estimate of how well the model is likely to perform on new data in the real world. Without a test set, it is difficult to know whether a model is simply memorizing the training data (overfitting) or whether it is truly learning the underlying patterns in the data (generalizing).\n",
    "\n",
    "- By evaluating the performance of the model on the test set, we can get an estimate of its accuracy, precision, recall, F1 score, and other performance metrics. If the model performs well on the test set, it is likely to perform well on new, unseen data. If it performs poorly, it may be necessary to revisit the model architecture, hyperparameters, or other aspects of the training process.\n",
    "\n",
    "- It's important to note that the test set should be kept separate from the training and validation sets to avoid any leakage of information and to ensure an unbiased estimate of the model's performance. It is also important to not use the test set for any purpose other than evaluating the final model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bcf320",
   "metadata": {},
   "source": [
    "### 17.What is a validation set's purpose?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e89343",
   "metadata": {},
   "source": [
    "- The purpose of a validation set in machine learning is to provide a means for tuning the hyperparameters of a model and assessing its performance during training.\n",
    "\n",
    "- During the training process, the model is optimized on the training set by updating its parameters to minimize the loss function. However, the model's performance on the training set is not necessarily indicative of its performance on new, unseen data. Therefore, we need a validation set to evaluate the model's generalization performance during training and to tune its hyperparameters.\n",
    "\n",
    "- The validation set is used to evaluate the model after each training epoch, and the performance metrics are used to adjust the hyperparameters of the model, such as the learning rate, regularization strength, or number of hidden units. This process is called hyperparameter tuning or model selection. By evaluating the model on the validation set, we can determine the best combination of hyperparameters that optimize the model's performance on both the training and validation sets.\n",
    "\n",
    "- It's important to note that the validation set should be separate from the training set and the test set to avoid any leakage of information and to ensure an unbiased estimate of the model's performance. Also, the validation set should not be used for training the model or adjusting its parameters, as this can lead to overfitting. Instead, it should be used only for tuning hyperparameters and assessing the model's generalization performance during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62bfbe8",
   "metadata": {},
   "source": [
    "### 18.What precisely is the train-dev kit, when will you need it, how do you put it to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae12ff6",
   "metadata": {},
   "source": [
    "- The train-dev kit, also known as the development set or the validation set, is a dataset that is used to tune the model's hyperparameters and evaluate its performance during the development phase of a machine learning project. It is a subset of the original training set that is held out for this purpose.\n",
    "\n",
    "- The purpose of the train-dev kit is to provide a more fine-grained evaluation of the model's performance during development, beyond what the validation set can provide. It allows us to detect issues such as data drift, model overfitting, and selection bias that may not be apparent when using only the validation set.\n",
    "\n",
    "- To use the train-dev kit, we first split the original training dataset into a training set, a validation set, and a train-dev kit. The training set is used to train the model, the validation set is used to tune the hyperparameters and evaluate the model's performance during training, and the train-dev kit is used to further evaluate the model's performance and detect any issues that may arise during development.\n",
    "\n",
    "- During development, we can evaluate the model's performance on the train-dev kit at regular intervals, along with the validation set, to ensure that it is not overfitting the training data and that it is still performing well on new, unseen data. If the model's performance on the train-dev kit is significantly worse than its performance on the validation set, this may indicate that the model is overfitting or that there is some other issue with the training data that needs to be addressed.\n",
    "\n",
    "- It's important to note that the train-dev kit should not be used for final model evaluation or hyperparameter tuning. Instead, it should be used only during the development phase to help diagnose and fix issues with the model or the training data. The final model evaluation should be performed on a separate test set that is held out from the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c8d827",
   "metadata": {},
   "source": [
    "### 19.What could go wrong if you use the test set to tune hyperparameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef7215a",
   "metadata": {},
   "source": [
    "- If you use the test set to tune hyperparameters, it can lead to overfitting and biased model selection, which can result in a model that performs poorly on new, unseen data. This is because the test set is meant to be a completely independent dataset that is used only for final model evaluation after the model has been fully trained and all hyperparameters have been selected.\n",
    "\n",
    "- If you use the test set to tune hyperparameters, you are essentially using the test set as a part of the training process, which can lead to overfitting. By repeatedly evaluating the model's performance on the test set during hyperparameter tuning, you are essentially leaking information about the test set into the model, which can bias the hyperparameter selection and lead to a model that is over-optimized for the test set.\n",
    "\n",
    "- Furthermore, using the test set for hyperparameter tuning violates the principle of unbiased model evaluation. The test set is meant to be completely independent of the training process and should not be used for anything other than final model evaluation. By using the test set for hyperparameter tuning, you are essentially contaminating it with information from the training process, which can lead to biased model selection and inaccurate estimates of model performance on new, unseen data.\n",
    "\n",
    "- To avoid these issues, it's important to set aside a separate validation set for hyperparameter tuning and use the test set only for final model evaluation after the model and all hyperparameters have been fully selected using the validation set. This ensures that the test set remains completely independent and unbiased and provides an accurate estimate of the model's performance on new, unseen data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
